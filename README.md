# Banking Analytics Workbench (BAW)

A comprehensive, end-to-end **local** retail banking analytics platform built with modern data stack technologies. This project demonstrates a complete banking analytics pipeline from data generation through machine learning insights, all running locally on synthetic data.

## ğŸ–¼ï¸ Dashboard Screenshots

### ğŸ¯ **Main Dashboard Overview**
![Dashboard Overview](screenshots/01-dashboard.png)
*Comprehensive overview of the Banking Analytics Workbench with enhanced KPI cards and modern glass-morphism design*

### ğŸ“Š **Key Performance Indicators**
![KPI Cards](screenshots/02-kpi.png)
*Interactive KPI cards with trend indicators, hover effects, and beautiful gradient styling*

### ğŸ“ˆ **Transaction Analytics**
![Transaction Analytics](screenshots/03-transactions.png)
*Multi-panel transaction visualization showing volume trends and transaction counts over time*

### ğŸ§ **ATM Cash Demand Forecast**
![ATM Forecast](screenshots/04-atm.png)
*3D surface visualization of ATM cash demand forecasting with interactive controls*

### ğŸ•µï¸ **Fraud Detection & Risk Analysis**
![Fraud Detection](screenshots/05-fraud.png)
*Advanced fraud detection dashboard with risk scoring, alerts, and distribution analysis*

---

## ğŸš€ Project Overview

BAW is designed to showcase best practices in banking analytics by providing:
- **Synthetic Data Generation**: Realistic banking data for development and testing
- **Modern Data Stack**: DuckDB + dbt + Airflow + Python ML stack
- **End-to-End Pipeline**: From raw data ingestion to ML-powered insights
- **Banking-Specific Analytics**: Fraud detection, churn prediction, ATM demand forecasting
- **Interactive Dashboards**: Streamlit-based visualization and exploration tools

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Data     â”‚    â”‚   DuckDB        â”‚    â”‚   dbt Models    â”‚
â”‚   (CSV Files)  â”‚â”€â”€â”€â–¶â”‚   Warehouse     â”‚â”€â”€â”€â–¶â”‚   (Staging,     â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚    Marts)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ML Models    â”‚    â”‚   Airflow       â”‚    â”‚   Streamlit     â”‚
â”‚   (Fraud,      â”‚â—€â”€â”€â”€â”‚   DAGs          â”‚â—€â”€â”€â”€â”‚   Dashboard     â”‚
â”‚    Churn,      â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚    ATM)        â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Data Models

### Source Tables
- **accounts.csv**: Customer account information
- **customers.csv**: Customer demographics and profiles
- **transactions.csv**: Financial transaction records
- **atm_withdrawals.csv**: ATM usage patterns
- **branches.csv**: Branch location and metadata
- **digital_sessions.csv**: Online banking session logs
- **support_tickets.csv**: Customer service interactions

### dbt Model Structure
```
models/
â”œâ”€â”€ staging/           # Raw data cleaning and standardization
â”‚   â”œâ”€â”€ stg_accounts.sql
â”‚   â”œâ”€â”€ stg_customers.sql
â”‚   â”œâ”€â”€ stg_transactions.sql
â”‚   â””â”€â”€ stg_branches.sql
â”œâ”€â”€ marts/            # Business-ready data models
â”‚   â”œâ”€â”€ dims/         # Dimension tables
â”‚   â”‚   â”œâ”€â”€ dim_customer.sql
â”‚   â”‚   â”œâ”€â”€ dim_account.sql
â”‚   â”‚   â””â”€â”€ dim_branch.sql
â”‚   â””â”€â”€ facts/        # Fact tables
â”‚       â”œâ”€â”€ fact_transactions.sql
â”‚       â”œâ”€â”€ fact_sessions.sql
â”‚       â””â”€â”€ fact_atm_demand.sql
â””â”€â”€ sources.yml       # Source definitions and metadata
```

## ğŸ¤– Machine Learning Capabilities

### Fraud Detection
- **Model**: Isolation Forest for anomaly detection
- **Features**: Transaction patterns, amount distributions, time-based anomalies
- **Output**: Fraud risk scores and flagged transactions

### Customer Churn Prediction
- **Model**: Baseline classification models
- **Features**: Account activity, transaction frequency, support interactions
- **Output**: Churn probability scores

### ATM Demand Forecasting
- **Model**: Time series forecasting
- **Features**: Historical withdrawal patterns, seasonal trends
- **Output**: Predicted ATM demand by location and time

## ğŸ› ï¸ Technology Stack

- **Data Storage**: DuckDB (local analytical database)
- **Data Transformation**: dbt (data build tool)
- **Workflow Orchestration**: Apache Airflow
- **Machine Learning**: Python (scikit-learn, pandas, numpy)
- **Data Visualization**: Streamlit
- **Data Quality**: Custom validation scripts
- **Version Control**: Git with dbt snapshots

## ğŸ“‹ Prerequisites

- Python 3.8+
- pip package manager
- Git
- Basic knowledge of SQL and Python

## ğŸš€ Quick Start

### 1. Environment Setup
```bash
# Clone the repository
git clone <repository-url>
cd rbc-baw

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# macOS/Linux:
source .venv/bin/activate
# Windows PowerShell:
# $env:DBT_PROFILES_DIR="$PWD/.venv\Scripts\Activate.ps1"

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

### 2. Configuration
```bash
# Set dbt profiles directory
# macOS/Linux:
export DBT_PROFILES_DIR=$(pwd)/.dbt
# Windows PowerShell:
# $env:DBT_PROFILES_DIR="$PWD/.dbt"
```

### 3. Data Pipeline Execution
```bash
# Generate synthetic banking data
python scripts/generate_data.py

# Load data into DuckDB warehouse
python scripts/load_to_duckdb.py

# Run dbt transformations
dbt deps && dbt run && dbt test
```

### 4. Machine Learning Models
```bash
# Run fraud detection
python scripts/fraud_isoforest.py

# Run churn prediction
python scripts/churn_baseline.py

# Run ATM demand forecasting
python scripts/atm_forecast.py
```

### 5. Launch Dashboard
```bash
streamlit run dashboards/app.py
```

## ğŸ“ Project Structure

```
rbc-baw/
â”œâ”€â”€ airflow/                 # Airflow DAGs for workflow orchestration
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ baw_pipeline.py
â”œâ”€â”€ dashboards/             # Streamlit dashboard application
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ data/                   # Data storage
â”‚   â”œâ”€â”€ raw/               # Source CSV files
â”‚   â”œâ”€â”€ warehouse/         # DuckDB database files
â”‚   â””â”€â”€ outputs/           # Generated outputs and results
â”œâ”€â”€ dbt_packages/          # dbt dependencies
â”œâ”€â”€ models/                 # dbt data models
â”‚   â”œâ”€â”€ staging/           # Data cleaning and standardization
â”‚   â”œâ”€â”€ marts/             # Business-ready models
â”‚   â””â”€â”€ sources.yml        # Source definitions
â”œâ”€â”€ scripts/                # Python utility scripts
â”‚   â”œâ”€â”€ generate_data.py   # Synthetic data generation
â”‚   â”œâ”€â”€ load_to_duckdb.py  # Data loading
â”‚   â”œâ”€â”€ fraud_isoforest.py # Fraud detection
â”‚   â”œâ”€â”€ churn_baseline.py  # Churn prediction
â”‚   â”œâ”€â”€ atm_forecast.py    # ATM demand forecasting
â”‚   â””â”€â”€ data_quality.py    # Data validation
â”œâ”€â”€ snapshots/              # dbt snapshots for change tracking
â”œâ”€â”€ target/                 # dbt compilation artifacts
â”œâ”€â”€ dbt_project.yml         # dbt project configuration
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md              # This file
```

## ğŸ”§ Configuration

### dbt Configuration
The project uses a local DuckDB profile configured in `.dbt/profiles.yml`:
```yaml
baw:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: data/warehouse/baw.duckdb
      schema: main
```

### Environment Variables
- `DBT_PROFILES_DIR`: Path to dbt profiles directory
- `PYTHONPATH`: Python path for module imports

## ğŸ“Š Data Quality & Testing

### dbt Tests
- **Generic tests**: Unique, not_null, relationships
- **Custom tests**: Business logic validation
- **Data quality checks**: Custom Python scripts for comprehensive validation

### Quality Metrics
- Data completeness
- Data consistency
- Business rule compliance
- Anomaly detection

## ğŸš€ Advanced Usage

### Custom Data Generation
Modify `scripts/generate_data.py` to:
- Adjust data volumes
- Change data distributions
- Add new data types
- Modify business rules

### Extending dbt Models
Add new models in the appropriate directory:
- `staging/` for new source tables
- `marts/dims/` for new dimensions
- `marts/facts/` for new fact tables

### Adding ML Models
Create new scripts in the `scripts/` directory following the existing pattern:
- Data loading and preprocessing
- Model training and evaluation
- Results storage and visualization

## ğŸ› Troubleshooting

### Common Issues

**dbt Connection Errors**
- Verify DuckDB database exists
- Check dbt profile configuration
- Ensure correct working directory

**Python Import Errors**
- Verify virtual environment activation
- Check PYTHONPATH configuration
- Install missing dependencies

**Data Quality Issues**
- Review data generation parameters
- Check dbt test results
- Validate source data integrity

### Debug Mode
Enable verbose logging in dbt:
```bash
dbt run --verbose
dbt test --verbose
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Built with modern data stack technologies
- Inspired by real-world banking analytics challenges
- Designed for educational and development purposes

## ğŸ“ Support

For questions or issues:
- Check the troubleshooting section
- Review dbt and DuckDB documentation
- Open an issue in the repository

---

**Happy Banking Analytics! ğŸ¦ğŸ“Š**
